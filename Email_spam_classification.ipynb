{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of spam_files :  501\n",
      "Length of ham_files :  2551\n"
     ]
    }
   ],
   "source": [
    "ham_files = [name for name in sorted(os.listdir(\"./Dataset/ham/\"))]\n",
    "spam_files = [name for name in sorted(os.listdir(\"./Dataset/spam//\"))]\n",
    "print(\"Length of spam_files : \",len(spam_files))\n",
    "print(\"Length of ham_files : \",len(ham_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_content(spam_or_not, f_name):\n",
    "    direc = \"./Dataset/spam\" if spam_or_not else \"./Dataset/ham\"\n",
    "    with open(os.path.join(direc, f_name), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "    \n",
    "ham_emails = [ email_content(spam_or_not=False, f_name=name) for name in ham_files]\n",
    "spam_emails = [ email_content(spam_or_not=True, f_name=name) for name in spam_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text/plain'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ham_emails[0].get_content_type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_struc(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    email_payload= email.get_payload()\n",
    "    if isinstance(email_payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([email_struc(sub_email) for sub_email in email_payload]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = email_struc(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_structure = structures_counter(ham_emails)\n",
    "spam_structure = structures_counter(spam_emails)\n",
    "for email in spam_emails:\n",
    "    if email_struc(email) == 'text/html':\n",
    "        testEmail = email\n",
    "        break\n",
    "def html_to_plain(email):\n",
    "    try:\n",
    "        soup = BeautifulSoup(email.get_content(), 'html.parser')\n",
    "        return soup.text.replace('\\n\\n','')\n",
    "    except:\n",
    "        return \"empty\"\n",
    "\n",
    "def email_to_plain(email):\n",
    "    struct = email_struc(email)\n",
    "    for part in email.walk():\n",
    "        partContentType = part.get_content_type()\n",
    "        if partContentType not in ['text/plain','text/html']:\n",
    "            continue\n",
    "        try:\n",
    "            partContent = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            partContent = str(part.get_payload())\n",
    "        if partContentType == 'text/plain':\n",
    "            return partContent\n",
    "        else:\n",
    "            return html_to_plain(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig=[]\n",
    "ham=[]\n",
    "spam=[]\n",
    "import pandas as pd \n",
    "import re\n",
    "count=0\n",
    "for text in ham_emails:\n",
    "    text=email_to_plain(text)\n",
    "    try:\n",
    "        text = text.replace('.','')\n",
    "        text = text.replace(':','')\n",
    "        text = text.replace(',','')\n",
    "        text = text.replace('!','')\n",
    "        text = text.replace('?','')\n",
    "        text=text.replace('\\n','')\n",
    "        text=text.replace('\\t','')\n",
    "        ham.append(text)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "for text in spam_emails:\n",
    "    text=email_to_plain(text)\n",
    "    try:\n",
    "        text = text.replace('.','')\n",
    "        text = text.replace(':','')\n",
    "        text = text.replace(',','')\n",
    "        text = text.replace('!','')\n",
    "        text = text.replace('?','')\n",
    "        text = text.replace('\\n','')\n",
    "        text=text.replace('\\t','')\n",
    "        spam.append(text)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "label=[0]*len(ham)+[1]*len(spam)\n",
    "# Extract text\n",
    "texts = ham+spam \n",
    "# Extract target\n",
    "target = label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "wl=50000\n",
    "tokenizer=Tokenizer(wl,filters='!@#$%^&*{}]~`[',lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text=tokenizer.texts_to_sequences(texts)\n",
    "msl=250\n",
    "text=pad_sequences(text,msl)\n",
    "from keras.layers import Embedding, LSTM,Dense,Dropout,Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y=to_categorical(target)\n",
    "x_train,x_test,y_train,y_test=train_test_split(text,y,test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(50000,64,input_length=250))\n",
    "model.add(LSTM(64,input_shape=(250,10),activation='tanh'))\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1708 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1708/1708 [==============================] - 35s 20ms/step - loss: 0.2502 - accuracy: 0.9122 - val_loss: 0.1167 - val_accuracy: 0.9578\n",
      "Epoch 2/10\n",
      "1708/1708 [==============================] - 34s 20ms/step - loss: 0.0215 - accuracy: 0.9947 - val_loss: 0.0604 - val_accuracy: 0.9836\n",
      "Epoch 3/10\n",
      "1708/1708 [==============================] - 33s 19ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0947 - val_accuracy: 0.9766\n",
      "Epoch 4/10\n",
      "1708/1708 [==============================] - 32s 19ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0907 - val_accuracy: 0.9742\n",
      "Epoch 5/10\n",
      "1708/1708 [==============================] - 33s 19ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0792 - val_accuracy: 0.9813\n",
      "Epoch 6/10\n",
      "1708/1708 [==============================] - 34s 20ms/step - loss: 2.8972e-04 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9836\n",
      "Epoch 7/10\n",
      "1708/1708 [==============================] - 33s 19ms/step - loss: 1.5638e-04 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9859\n",
      "Epoch 8/10\n",
      "1708/1708 [==============================] - 32s 19ms/step - loss: 1.1789e-04 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 0.9859\n",
      "Epoch 9/10\n",
      "1708/1708 [==============================] - 34s 20ms/step - loss: 8.6130e-05 - accuracy: 1.0000 - val_loss: 0.0792 - val_accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "1708/1708 [==============================] - 33s 19ms/step - loss: 6.2803e-05 - accuracy: 1.0000 - val_loss: 0.0802 - val_accuracy: 0.9883\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=10, validation_split=0.2 ,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916/916 [==============================] - 2s 2ms/step\n",
      "Test set\n",
      "  Loss: 0.038\n",
      "  Accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "y_test\n",
    "labels=[1,0]\n",
    "y_test_label=[]\n",
    "y_pred_label=[]\n",
    "\n",
    "for i in y_test:\n",
    "    y_test_label.append(labels[np.argmax(i)])\n",
    "for i in y_pred:\n",
    "    y_pred_label.append(labels[np.argmax(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "new_complaint = ['sdhfsdfhykldjfkl  h kkjhfsd jkhadk hudfhkjdfhj,h h ']\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "padded = pad_sequences(seq, maxlen=250)\n",
    "pred = model.predict(padded)\n",
    "labels = ['ham','spam']\n",
    "print( labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "new_complaint = ['todays weather is cool ,isnt it']\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "padded = pad_sequences(seq, maxlen=250)\n",
    "pred = model.predict(padded)\n",
    "labels = ['ham','spam']\n",
    "print( labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
